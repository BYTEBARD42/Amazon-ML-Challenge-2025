{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7e646",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "# ============================================\n",
    "# SMAPE METRIC\n",
    "# ============================================\n",
    "def smape(y_true, y_pred):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    smape_val = np.mean(diff / (denominator + 1e-10)) * 100\n",
    "    return smape_val\n",
    "\n",
    "class SMAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SMAPELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2.0\n",
    "        diff = torch.abs(y_true - y_pred)\n",
    "        smape_loss = torch.mean(diff / (denominator + 1e-10)) * 100\n",
    "        return smape_loss\n",
    "\n",
    "# ============================================\n",
    "# DATA PREPROCESSING\n",
    "# ============================================\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, vocab_size=10000, max_length=50):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        \n",
    "    def clean_text(self, text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def build_vocab(self, texts):\n",
    "        all_words = [word for text in texts for word in self.clean_text(text).split()]\n",
    "        word_freq = Counter(all_words)\n",
    "        most_common = word_freq.most_common(self.vocab_size - 2)\n",
    "        \n",
    "        for idx, (word, _) in enumerate(most_common, start=2):\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "            \n",
    "        print(f\"Vocabulary size: {len(self.word2idx)}\")\n",
    "        return self\n",
    "    \n",
    "    def text_to_sequence(self, text):\n",
    "        cleaned = self.clean_text(text)\n",
    "        words = cleaned.split()\n",
    "        sequence = [self.word2idx.get(word, 1) for word in words]\n",
    "        \n",
    "        if len(sequence) < self.max_length:\n",
    "            sequence += [0] * (self.max_length - len(sequence))\n",
    "        else:\n",
    "            sequence = sequence[:self.max_length]\n",
    "        return sequence\n",
    "    \n",
    "    def texts_to_sequences(self, texts):\n",
    "        return [self.text_to_sequence(text) for text in texts]\n",
    "    \n",
    "    def save(self, filepath):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "# ============================================\n",
    "# DATASET CLASS\n",
    "# ============================================\n",
    "class PriceDataset(Dataset):\n",
    "    def __init__(self, sequences, prices=None):\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.prices = torch.FloatTensor(prices) if prices is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.prices is not None:\n",
    "            return self.sequences[idx], self.prices[idx]\n",
    "        else:\n",
    "            return self.sequences[idx]\n",
    "\n",
    "# ============================================\n",
    "# MODEL 1: Enhanced Bi-LSTM with Multi-Head Attention\n",
    "# ============================================\n",
    "class BiLSTMWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, \n",
    "                 num_layers=3, dropout=0.3, num_heads=4):\n",
    "        super(BiLSTMWithAttention, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Bi-LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim * 2 // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.key = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.value = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def multi_head_attention(self, lstm_out):\n",
    "        batch_size, seq_len, hidden_size = lstm_out.size()\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.query(lstm_out).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(lstm_out).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(lstm_out).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attended = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Pool over sequence\n",
    "        context_vector = torch.mean(attended, dim=1)\n",
    "        \n",
    "        return context_vector\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "        \n",
    "        # Multi-head attention\n",
    "        context_vector = self.multi_head_attention(lstm_out)\n",
    "        \n",
    "        # Fully connected layers with batch norm\n",
    "        x = self.relu(self.bn1(self.fc1(context_vector)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# ============================================\n",
    "# MODEL 2: CNN-LSTM Hybrid\n",
    "# ============================================\n",
    "class CNNLSTMHybrid(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, \n",
    "                 num_layers=2, dropout=0.3):\n",
    "        super(CNNLSTMHybrid, self).__init__()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Multiple CNN filters with different kernel sizes\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(embedding_dim, 128, kernel_size=4, padding=2)\n",
    "        self.conv3 = nn.Conv1d(embedding_dim, 128, kernel_size=5, padding=2)\n",
    "        \n",
    "        self.bn_conv1 = nn.BatchNorm1d(128)\n",
    "        self.bn_conv2 = nn.BatchNorm1d(128)\n",
    "        self.bn_conv3 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Pool CNN outputs\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        # Bi-LSTM on CNN features\n",
    "        self.lstm = nn.LSTM(\n",
    "            384,  # 128 * 3 from three CNN filters\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2 + 384, 256)  # Concatenate LSTM + CNN features\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        # CNN expects (batch, channels, length)\n",
    "        embedded_transposed = embedded.transpose(1, 2)  # (batch, embedding_dim, seq_len)\n",
    "        \n",
    "        # Apply multiple CNN filters\n",
    "        conv1_out = self.relu(self.bn_conv1(self.conv1(embedded_transposed)))\n",
    "        conv2_out = self.relu(self.bn_conv2(self.conv2(embedded_transposed)))\n",
    "        conv3_out = self.relu(self.bn_conv3(self.conv3(embedded_transposed)))\n",
    "        \n",
    "        # Concatenate CNN outputs\n",
    "        conv_out = torch.cat([conv1_out, conv2_out, conv3_out], dim=1)  # (batch, 384, seq_len)\n",
    "        \n",
    "        # Global max pooling for CNN features\n",
    "        cnn_features = self.pool(conv_out).squeeze(-1)  # (batch, 384)\n",
    "        \n",
    "        # Transpose back for LSTM\n",
    "        conv_out_transposed = conv_out.transpose(1, 2)  # (batch, seq_len, 384)\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        lstm_out, _ = self.lstm(conv_out_transposed)\n",
    "        \n",
    "        # Attention on LSTM output\n",
    "        attention_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        lstm_context = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Concatenate CNN and LSTM features\n",
    "        combined = torch.cat([cnn_features, lstm_context], dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.bn1(self.fc1(combined)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# ============================================\n",
    "# MODEL 3: Advanced CNN-BiLSTM with Residual Connections\n",
    "# ============================================\n",
    "class CNNBiLSTMResidual(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, \n",
    "                 num_layers=2, dropout=0.3):\n",
    "        super(CNNBiLSTMResidual, self).__init__()\n",
    "        \n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embedding_dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # CNN Block 1\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 128, kernel_size=3, padding=1)\n",
    "        self.bn_conv1 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # CNN Block 2 (residual)\n",
    "        self.conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn_conv2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        # CNN Block 3 (residual)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn_conv3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            256,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Self-attention\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        # CNN blocks with residual connections\n",
    "        x_cnn = embedded.transpose(1, 2)  # (batch, embedding_dim, seq_len)\n",
    "        \n",
    "        # Block 1\n",
    "        x_cnn = self.relu(self.bn_conv1(self.conv1(x_cnn)))\n",
    "        \n",
    "        # Block 2 (with residual)\n",
    "        residual = x_cnn\n",
    "        x_cnn = self.relu(self.bn_conv2(self.conv2(x_cnn)))\n",
    "        x_cnn = x_cnn + residual  # Residual connection\n",
    "        \n",
    "        # Block 3\n",
    "        x_cnn = self.relu(self.bn_conv3(self.conv3(x_cnn)))\n",
    "        \n",
    "        # Back to sequence format for LSTM\n",
    "        x_cnn = x_cnn.transpose(1, 2)  # (batch, seq_len, 256)\n",
    "        \n",
    "        # Bi-LSTM\n",
    "        lstm_out, _ = self.lstm(x_cnn)\n",
    "        \n",
    "        # Self-attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Pool over sequence\n",
    "        pooled = torch.mean(attn_out, dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.relu(self.bn1(self.fc1(pooled)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# ============================================\n",
    "# TRAINING PIPELINE\n",
    "# ============================================\n",
    "class PricePredictionTrainer:\n",
    "    def __init__(self, model, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.best_smape = float('inf')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "    def train(self, train_loader, val_loader, epochs=100, lr=0.001, patience=15):\n",
    "        criterion = SMAPELoss()\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', patience=5, factor=0.5, verbose=True\n",
    "        )\n",
    "        \n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for sequences, prices in train_loader:\n",
    "                sequences, prices = sequences.to(self.device), prices.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(sequences)\n",
    "                loss = criterion(outputs, prices)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_smape = self._validate(val_loader, criterion)\n",
    "            scheduler.step(val_smape)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_smape < self.best_smape:\n",
    "                self.best_smape = val_smape\n",
    "                torch.save({'model_state_dict': self.model.state_dict()}, '/kaggle/input/amazon-ml-challenge-2025/best_model.pth')\n",
    "                patience_counter = 0\n",
    "                print(f'Epoch [{epoch+1}/{epochs}] â­ Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val SMAPE: {val_smape:.4f}')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if (epoch + 1) % 5 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val SMAPE: {val_smape:.4f}')\n",
    "            \n",
    "            # Early stopping\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Load best model\n",
    "        checkpoint = torch.load('/kaggle/input/amazon-ml-challenge-2025/best_model.pth')\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"\\nðŸŽ¯ Best validation SMAPE: {self.best_smape:.4f}\")\n",
    "    \n",
    "    def _validate(self, val_loader, criterion):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds, all_targets = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sequences, prices in val_loader:\n",
    "                sequences, prices = sequences.to(self.device), prices.to(self.device)\n",
    "                outputs = self.model(sequences)\n",
    "                val_loss += criterion(outputs, prices).item()\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "                all_targets.extend(prices.cpu().numpy())\n",
    "        \n",
    "        return val_loss / len(val_loader), smape(np.array(all_targets), np.array(all_preds))\n",
    "\n",
    "    def predict(self, test_loader):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        with torch.no_grad():\n",
    "            for sequences in test_loader:\n",
    "                sequences = sequences.to(self.device)\n",
    "                outputs = self.model(sequences)\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "        return np.array(all_preds)\n",
    "\n",
    "# ============================================\n",
    "# MAIN PIPELINE FUNCTIONS\n",
    "# ============================================\n",
    "def train_pipeline(train_df, model_type='bilstm', val_split=0.2):\n",
    "    \"\"\"\n",
    "    model_type: 'bilstm', 'cnn_lstm', or 'cnn_bilstm_res'\n",
    "    \"\"\"\n",
    "    print(\"=\"*50 + f\"\\nTRAINING PIPELINE - {model_type.upper()}\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\n1. Preparing data...\")\n",
    "    train_df = train_df.dropna(subset=['catalog_content', 'price'])\n",
    "    descriptions, prices = train_df['catalog_content'].values, train_df['price'].values\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(descriptions, prices, test_size=val_split, random_state=42)\n",
    "    print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "    \n",
    "    # Build vocabulary\n",
    "    print(\"\\n2. Building vocabulary...\")\n",
    "    preprocessor = TextPreprocessor(vocab_size=15000, max_length=100)\n",
    "    preprocessor.build_vocab(X_train)\n",
    "    \n",
    "    X_train_seq = preprocessor.texts_to_sequences(X_train)\n",
    "    X_val_seq = preprocessor.texts_to_sequences(X_val)\n",
    "    \n",
    "    train_loader = DataLoader(PriceDataset(X_train_seq, y_train), batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(PriceDataset(X_val_seq, y_val), batch_size=128)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(f\"\\n3. Initializing {model_type.upper()} model...\")\n",
    "    vocab_size = len(preprocessor.word2idx)\n",
    "    \n",
    "    if model_type == 'bilstm':\n",
    "        model = BiLSTMWithAttention(vocab_size, embedding_dim=128, hidden_dim=256, num_layers=3)\n",
    "    elif model_type == 'cnn_lstm':\n",
    "        model = CNNLSTMHybrid(vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2)\n",
    "    elif model_type == 'cnn_bilstm_res':\n",
    "        model = CNNBiLSTMResidual(vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n4. Training model...\")\n",
    "    trainer = PricePredictionTrainer(model)\n",
    "    trainer.train(train_loader, val_loader, epochs=100, lr=0.001, patience=15)\n",
    "    \n",
    "    preprocessor.save('preprocessor.pkl')\n",
    "    print(\"\\nâœ… Preprocessor and best model saved.\")\n",
    "    \n",
    "def prediction_pipeline(test_df, model_type='bilstm', \n",
    "                       preprocessor_path='preprocessor.pkl', \n",
    "                       model_path='/kaggle/input/amazon-ml-challenge-2025/best_model.pth'):\n",
    "    print(\"=\"*50 + f\"\\nPREDICTION PIPELINE - {model_type.upper()}\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Load preprocessor\n",
    "    print(\"\\n1. Loading preprocessor and model...\")\n",
    "    preprocessor = TextPreprocessor.load(preprocessor_path)\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_descriptions = test_df['catalog_content'].values\n",
    "    test_sequences = preprocessor.texts_to_sequences(test_descriptions)\n",
    "    test_loader = DataLoader(PriceDataset(test_sequences), batch_size=128)\n",
    "    \n",
    "    # Load model\n",
    "    vocab_size = len(preprocessor.word2idx)\n",
    "    \n",
    "    if model_type == 'bilstm':\n",
    "        model = BiLSTMWithAttention(vocab_size, embedding_dim=128, hidden_dim=256, num_layers=3)\n",
    "    elif model_type == 'cnn_lstm':\n",
    "        model = CNNLSTMHybrid(vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2)\n",
    "    elif model_type == 'cnn_bilstm_res':\n",
    "        model = CNNBiLSTMResidual(vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2)\n",
    "    \n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\n2. Making predictions...\")\n",
    "    trainer = PricePredictionTrainer(model)\n",
    "    predictions = trainer.predict(test_loader)\n",
    "    \n",
    "    # Create submission\n",
    "    submission_df = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': predictions})\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nâœ… Predictions saved to 'submission.csv'\")\n",
    "\n",
    "# ============================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    train_df = pd.read_csv('/kaggle/input/amazon-ml-challenge-2025/train.csv')\n",
    "    \n",
    "    # Choose one model:\n",
    "    \n",
    "    # Option 1: Bi-LSTM with Multi-Head Attention (Best for long sequences)\n",
    "    train_pipeline(train_df, model_type='bilstm')\n",
    "    \n",
    "    # Option 2: CNN-LSTM Hybrid (Best for local patterns + sequences)\n",
    "    # train_pipeline(train_df, model_type='cnn_lstm')\n",
    "    \n",
    "    # Option 3: CNN-BiLSTM with Residual (Most advanced)\n",
    "    # train_pipeline(train_df, model_type='cnn_bilstm_res')\n",
    "    \n",
    "    # Predict\n",
    "    test_df = pd.read_csv('/kaggle/input/amazon-ml-challenge-2025/test.csv')\n",
    "    prediction_pipeline(test_df, model_type='bilstm')  # Match the model type used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07476031",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FINE-TUNING PIPELINE (Add this new function)\n",
    "# ============================================\n",
    "def finetune_pipeline(train_df,\n",
    "                      preprocessor_path='preprocessor.pkl',\n",
    "                      model_path='best_model.pth',\n",
    "                      val_split=0.2):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained model and its preprocessor to continue training (fine-tune).\n",
    "    \n",
    "    Args:\n",
    "        train_df (pd.DataFrame): DataFrame with data for fine-tuning.\n",
    "        preprocessor_path (str): Path to the saved preprocessor from initial training.\n",
    "        model_path (str): Path to the saved pre-trained model (.pth).\n",
    "        val_split (float): The proportion of the dataset to use for validation.\n",
    "    \"\"\"\n",
    "    print(\"=\"*50 + \"\\nFINE-TUNING PIPELINE\\n\" + \"=\"*50)\n",
    "    \n",
    "    # --- 1. Data Loading and Cleaning (same as before) ---\n",
    "    train_df = train_df.dropna(subset=['catalog_content', 'price'])\n",
    "    train_df['price'] = pd.to_numeric(train_df['price'], errors='coerce')\n",
    "    train_df = train_df.dropna(subset=['price'])\n",
    "    \n",
    "    descriptions, prices = train_df['catalog_content'].values, train_df['price'].values\n",
    "    X_train, X_val, y_train, y_val = train_test_split(descriptions, prices, test_size=val_split, random_state=42)\n",
    "    print(f\"Fine-tuning with {len(X_train)} samples, Validating with {len(X_val)} samples.\")\n",
    "\n",
    "    # --- 2. KEY CHANGE: Load the existing preprocessor ---\n",
    "    # We DO NOT build a new vocabulary. We must use the one the model already knows.\n",
    "    print(f\"Loading preprocessor from {preprocessor_path}...\")\n",
    "    preprocessor = TextPreprocessor.load(preprocessor_path)\n",
    "    \n",
    "    # Process text using the loaded preprocessor\n",
    "    X_train_seq = preprocessor.texts_to_sequences(X_train)\n",
    "    X_val_seq = preprocessor.texts_to_sequences(X_val)\n",
    "    \n",
    "    train_loader = DataLoader(PriceDataset(X_train_seq, y_train), batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(PriceDataset(X_val_seq, y_val), batch_size=128)\n",
    "\n",
    "    # --- 3. KEY CHANGE: Instantiate model and load pre-trained weights ---\n",
    "    vocab_size = len(preprocessor.word2idx)\n",
    "    model = BiLSTMWithAttention(vocab_size, embedding_dim=128, hidden_dim=256, num_layers=3)\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Loading model weights from {model_path}...\")\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device(device))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "    # --- 4. Training with a lower learning rate ---\n",
    "    # The trainer class can be reused as is.\n",
    "    trainer = PricePredictionTrainer(model)\n",
    "    \n",
    "    # KEY CHANGE: Use a smaller learning rate for fine-tuning to make small adjustments.\n",
    "    print(\"Starting fine-tuning process with a lower learning rate...\")\n",
    "    # Note the much smaller learning rate (lr)\n",
    "    trainer.train(train_loader, val_loader, epochs=50, lr=5e-5, patience=10)\n",
    "    \n",
    "    # --- 5. Save the fine-tuned model to a NEW file ---\n",
    "    # This avoids overwriting your original best model.\n",
    "    trainer.model.save_state_dict('best_model.pth')\n",
    "    print(\"\\nâœ… Fine-tuning complete. New model saved to 'best_model.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5832bc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================\n",
    "if _name_ == \"_main_\":\n",
    "    # You can fine-tune on the same training data or a new, similar dataset.\n",
    "    # Here, we use the original training data.\n",
    "    train_df = pd.read_csv('train.csv')\n",
    "\n",
    "    # Run the new fine-tuning function\n",
    "    finetune_pipeline(train_df)\n",
    "    \n",
    "    # You can now use 'finetuned_model.pth' for predictions\n",
    "    # test_df = pd.read_csv('test.csv')\n",
    "    # prediction_pipeline(test_df, model_path='finetuned_model.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
